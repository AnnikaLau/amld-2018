{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1\n",
    "\n",
    "In this session, we will look at the wikileaks dataset and learn how to start gathering statistics about the dataset, preprocess the emails and extract useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading JSON file\n",
    "\n",
    "In the folder you will find a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_data = '../../data/clean_json.json'\n",
    "\n",
    "def load_json_data(path_to_file):\n",
    "    data_DF = pd.read_json(path_to_file,encoding='ascii')\n",
    "    data_DF['from'] = data_DF['from'].str.lower()\n",
    "    data_DF['body'] = data_DF['body'].apply(lambda x: \" \".join(str(x).split()))\n",
    "    return data_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset from data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json_data(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "import string\n",
    "\n",
    "stop_words_list = stopwords.words('english') + list(string.punctuation) #TODO: add other words?\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        self.user_emails = list(set(self.data['from']))\n",
    "        self._generate_email2name()\n",
    "        self.word_count = Counter()\n",
    "\n",
    "    def _generate_email2name(self):\n",
    "        self.EMAIL2NAME = defaultdict(list) # in case there are aliases\n",
    "        user_emails = self.data['from']\n",
    "        user_names = self.data['from_name']\n",
    "        receivers = self.data['to']\n",
    "        for email, name in zip(list(user_emails),list(user_names)):\n",
    "            email = email.lower()\n",
    "            name = name.replace('\"','')\n",
    "            if name not in self.EMAIL2NAME[email]:\n",
    "                self.EMAIL2NAME[email].append(name)\n",
    "\n",
    "        receivers_emails = []\n",
    "        for receiver in list(receivers):\n",
    "            for name, email in receiver:\n",
    "                email = email.lower()\n",
    "                name = name.replace('\"','')\n",
    "                if name not in self.EMAIL2NAME[email]:\n",
    "                    self.EMAIL2NAME[email].append(name)\n",
    "\n",
    "    def get_top_spammers(self, ntop=9999):\n",
    "        print(\"Count \\t Email \\t \\t \\t Name\")\n",
    "        list_spammers = []\n",
    "        printout = 0\n",
    "        for a in self.data.groupby(self.data['from'])['from'].count()\\\n",
    "                                        .reset_index(name='count') \\\n",
    "                                        .sort_values(['count'], ascending=False)\\\n",
    "                                        .iterrows():\n",
    "                _, email = a\n",
    "                if printout < ntop:\n",
    "                    print(\"%i \\t %s \\t %s\" %(email['count'],email['from'],self.EMAIL2NAME[email['from']][0]))\n",
    "                    printout += 1\n",
    "                    list_spammers.append([email['count'],email['from'],self.EMAIL2NAME[email['from']][0]])\n",
    "        return list_spammers\n",
    "            \n",
    "    def get_total_vocabulary(self):\n",
    "        #returns a dict of emails and their respective vocab\n",
    "        self.vocabulary = self.data['body'].str.cat(sep=' ') + self.data['subject'].str.cat(sep=' ')\n",
    "        return self.vocabulary\n",
    "    \n",
    "    def get_vocabulary_count(self,stop_words=False):\n",
    "        if stop_words:\n",
    "            self.word_count = Counter([x for x in self.vocabulary.split(' ') if x not in stop_words_list])\n",
    "        else:\n",
    "            self.word_count = Counter([x for x in self.vocabulary.split(' ')])\n",
    "\n",
    "        return self.word_count\n",
    "    \n",
    "    def get_top_words(self,stop_words=False):\n",
    "        if len(self.word_count.keys())==0:\n",
    "            self.get_vocabulary_count(stop_words=stop_words)\n",
    "        print('Word \\t Count')\n",
    "        for a,b in self.word_count.most_common(20):\n",
    "            print('%s \\t %i)' %(a, b))\n",
    "        return self.word_count.most_common(20)\n",
    "        \n",
    "    def generate_reduced_dataset(self, list_of_users):\n",
    "        pass\n",
    "        #returns a smaller dataframe\n",
    "\n",
    "def plot_time(dataframe):\n",
    "    #new = dataframe[['date']]\n",
    "    #new['hour'] = # TODO \n",
    "    #new['hour'].hist(bins=24)\n",
    "    #plt.title('Emails per hour')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore this dataset a bit.\n",
    "\n",
    "1. For example, who sends out most emails?\n",
    "2. Which words are most common?\n",
    "3. Around when were emails most received (by day of the week and hour)\n",
    "\n",
    "In particular, how can we improve the output of question 2 (if the most common words aren't particularly interesting?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiate the dataset\n",
    "DataObject = Dataset(data)\n",
    "\n",
    "tab = DataObject.get_top_spammers(ntop=...)\n",
    "word_count = DataObject.get_top_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to get a feeling of what these people are talking about.\n",
    "\n",
    "In this example, we will do a simple topic mining model and use spacy to pick up on relevant entities.\n",
    "\n",
    "In particular:\n",
    "1. Aggregate the communication between two people\n",
    "1. Perform topic modelling on the subset exchanged emails\n",
    "2. Perform named entity extraction on the subset\n",
    "\n",
    "The output of this task is to find pairs of people and the keywords/topics they are talking about in their emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "list_of_entities = nlp.entity.cfg[u'actions']['1']\n",
    "relevant_entities = list_of_entities\n",
    "\n",
    "def clean_text(text):\n",
    "    return text\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            topics.append(feature_names[i])\n",
    "        \n",
    "    return topics\n",
    "\n",
    "def get_keywords(sentence):\n",
    "    keywords = defaultdict(list)\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in relevant_entities:\n",
    "            keywords[ent.label_].append(ent.text)\n",
    "    return keywords\n",
    "\n",
    "def get_topics(emails):\n",
    "    # eats a list of emails and returns 3 topics \n",
    "    # NMF is able to use tf-idf\n",
    "    temp = []\n",
    "    for em in emails:\n",
    "        try:\n",
    "            accum = [a for a in em[0].split('.')]\n",
    "            temp += accum\n",
    "        except:\n",
    "            continue\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=False)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(temp)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "    tf_vectorizer = CountVectorizer(stop_words='english', lowercase=False)\n",
    "    tf = tf_vectorizer.fit_transform(temp)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    no_topics = 5\n",
    "\n",
    "    # Run NMF\n",
    "    nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "    # Run LDA\n",
    "    lda = LatentDirichletAllocation(n_components=no_topics, max_iter=10, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "    no_top_words = 3\n",
    "    topics1 = display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "    topics2 = display_topics(lda, tf_feature_names, no_top_words)\n",
    "    return topics1 + topics2\n",
    "\n",
    "class user(object):\n",
    "    def __init__(self, data, email):\n",
    "        self.user = email\n",
    "        self.emails = data.data.loc[data.data['from']==self.user]['body']\n",
    "        self.vocabulary_raw = self.get_vocabulary(data)\n",
    "        self.keywords = defaultdict(list)\n",
    "        self.correspondents_count = Counter()\n",
    "        self.correspondents_emails = defaultdict(list)\n",
    "        self.correspondents_keywords = defaultdict(dict)\n",
    "        self.get_connections(data)\n",
    "        self.correspondents_topics = defaultdict(list)\n",
    "        self.get_topics_correspondents()\n",
    "        self.connections = self.correspondents_count.keys()\n",
    "\n",
    "    def get_vocabulary(self, data):\n",
    "        return data.data.loc[data.data['from']==self.user]['body'].str.cat(sep=' ')\n",
    "    \n",
    "    def get_connections(self,data):\n",
    "        # return person, number of emails, top entities\n",
    "        self.keywords_per_receiver = defaultdict(dict)\n",
    "        for row in data.data.loc[data.data['from']==self.user].itertuples():\n",
    "            indx, body, date, sender, from_name, subject, corres = row\n",
    "            #try:\n",
    "            if len(corres) == 0:\n",
    "                continue\n",
    "            for r in corres[0]:\n",
    "                    if '@' not in r:\n",
    "                        pass\n",
    "                    else:\n",
    "                        r = r.lower()\n",
    "                        self.correspondents_count[r] += 1\n",
    "                        self.correspondents_emails[r].append([clean_text(body)])\n",
    "                        keywords = get_keywords(clean_text(body))\n",
    "                        \n",
    "                        if r not in self.correspondents_keywords.keys():\n",
    "                            for key in relevant_entities:\n",
    "                                self.correspondents_keywords[r][key] = []\n",
    "                        for key in keywords.keys():\n",
    "                            if key in relevant_entities:\n",
    "                                self.correspondents_keywords[r][key] += keywords[key]\n",
    "                                \n",
    "        for receiver in self.correspondents_emails.keys():\n",
    "            for row in data.data.loc[data.data['from']==receiver].itertuples():\n",
    "                indx, body, date, sender, from_name, subject, corres = row\n",
    "                if len(corres) == 0:\n",
    "                    continue\n",
    "                if self.user not in corres[0]:\n",
    "                    continue\n",
    "                    \n",
    "                self.correspondents_emails[receiver].append(str(body))\n",
    "                keywords = get_keywords(body)\n",
    "                        \n",
    "                if receiver not in self.correspondents_keywords.keys():\n",
    "                    #instanciate dictionary\n",
    "                    for key in relevant_entities:\n",
    "                        self.correspondents_keywords[receiver][key] = []\n",
    "                        \n",
    "                for key in keywords.keys():\n",
    "                    if key in relevant_entities:\n",
    "                        self.correspondents_keywords[receiver][key] += keywords[key]\n",
    "    \n",
    "    def get_topics_correspondents(self):\n",
    "        for corres in self.correspondents_keywords.keys():\n",
    "            try:\n",
    "                topics = get_topics(userA.correspondents_emails[corres])\n",
    "            except:\n",
    "                topics = []\n",
    "            counter = Counter(topics)\n",
    "            self.correspondents_topics[corres] = counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose now we are interested in looking at a person in particular. For example, some names were particularly centered in the controversy, such as:\n",
    "\n",
    "Debbie Wasserman (email: hrtsleeve@gmail.com)     \n",
    "Brad Marshal (email: marshall@dnc.or)       \n",
    "Luis Miranda (mirandal@dnc.org) (he's just the top spammer :) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userA = user(DataObject,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the last part of this session, let's see if we can extract some interesting topics from the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle   \n",
    "    \n",
    "tab = DataObject.get_top_spammers(ntop=50)\n",
    "top_s = [a[1] for a in tab]\n",
    "\n",
    "graph = []\n",
    "for indx, email in enumerate(top_s[0:5]):\n",
    "    userA = user(DataObject,email)\n",
    "    for key in userA.correspondents_count.keys():\n",
    "        graph.append({'email': email, 'correspondent': key, 'topics': userA.correspondents_topics[key], 'keywords': userA.correspondents_keywords[key], 'count': userA.correspondents_count[key]})\n",
    "\n",
    "pickle.dump(graph, open('graph_topics_dict_t.pkl','wb'))\n",
    "a = pickle.load(open('graph_topics_dict_t.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_words(dictionary, exclude=[]):\n",
    "    all_words = dictionary['topics']\n",
    "    for key in list(dictionary['keywords'].keys()):\n",
    "        temp = []\n",
    "        if len(dictionary['email'])==0:\n",
    "            return \n",
    "        #all_words+= \n",
    "        temp = dictionary['keywords'][key]\n",
    "        if len(temp) < 2:\n",
    "            continue\n",
    "        count = Counter(temp)\n",
    "        print(count)\n",
    "        for a, b in count.most_common(1): #for example \n",
    "            print(a)\n",
    "            all_words.append(a)\n",
    "            \n",
    "    all_words = [a for a in all_words if a not in exclude]\n",
    "    \n",
    "    print('Email: ', dictionary['email'], 'To: ', dictionary['correspondent'],\\\n",
    "          'Words: ', all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for e in graph:\n",
    "    exclude_words = [DataObject.EMAIL2NAME[e['email']][0], DataObject.EMAIL2NAME[e['correspondent']][0], e['email'], e['correspondent'],\\\n",
    "                    ] + DataObject.EMAIL2NAME[e['email']][0].split(',') + DataObject.EMAIL2NAME[e['correspondent']][0].split(',')\n",
    "    get_top_words(e, exclude = exclude_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
