{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduce the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode (load / train)? load\n"
     ]
    }
   ],
   "source": [
    "# Import relevant packages and modules\n",
    "from util import *\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Prompt for mode\n",
    "mode = input('mode (load / train)? ')\n",
    "\n",
    "\n",
    "# Set file names\n",
    "file_train_instances = \"train_stances.csv\"\n",
    "file_train_bodies = \"train_bodies.csv\"\n",
    "file_test_instances = \"test_stances_unlabeled.csv\"\n",
    "file_test_bodies = \"test_bodies.csv\"\n",
    "file_predictions = 'predictions_test.csv'\n",
    "\n",
    "\n",
    "# Initialise hyperparameters\n",
    "r = random.Random()\n",
    "lim_unigram = 5000\n",
    "target_size = 4\n",
    "hidden_size = 100\n",
    "train_keep_prob = 0.6\n",
    "l2_alpha = 0.00001\n",
    "learn_rate = 0.01\n",
    "clip_ratio = 5\n",
    "batch_size_train = 500\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Load data sets\n",
    "raw_train = FNCData(file_train_instances, file_train_bodies)\n",
    "raw_test = FNCData(file_test_instances, file_test_bodies)\n",
    "n_train = len(raw_train.instances)\n",
    "\n",
    "\n",
    "# Process data sets\n",
    "train_set, train_stances, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer = \\\n",
    "    pipeline_train(raw_train, raw_test, lim_unigram=lim_unigram)\n",
    "feature_size = len(train_set[0])\n",
    "test_set = pipeline_test(raw_test, bow_vectorizer, tfreq_vectorizer, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "# Create placeholders\n",
    "features_pl = tf.placeholder(tf.float32, [None, feature_size], 'features')\n",
    "stances_pl = tf.placeholder(tf.int64, [None], 'stances')\n",
    "keep_prob_pl = tf.placeholder(tf.float32)\n",
    "\n",
    "# Infer batch size\n",
    "batch_size = tf.shape(features_pl)[0]\n",
    "\n",
    "# Define multi-layer perceptron\n",
    "hidden_layer = tf.nn.dropout(tf.nn.relu(tf.contrib.layers.linear(features_pl, hidden_size)), keep_prob=keep_prob_pl)\n",
    "logits_flat = tf.nn.dropout(tf.contrib.layers.linear(hidden_layer, target_size), keep_prob=keep_prob_pl)\n",
    "logits = tf.reshape(logits_flat, [batch_size, target_size])\n",
    "\n",
    "# Define L2 loss\n",
    "tf_vars = tf.trainable_variables()\n",
    "l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf_vars if 'bias' not in v.name]) * l2_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define overall loss\n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=stances_pl) + l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction\n",
    "softmaxed_logits = tf.nn.softmax(logits)\n",
    "predict = tf.arg_max(softmaxed_logits, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch, Loss =      9: 19318.357\n",
      "  Epoch, Loss =     19: 19151.643\n",
      "  Epoch, Loss =     29: 18706.768\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "if mode == 'train':\n",
    "\n",
    "    # Define optimiser\n",
    "    opt_func = tf.train.AdamOptimizer(learn_rate)\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tf_vars), clip_ratio)\n",
    "    opt_op = opt_func.apply_gradients(zip(grads, tf_vars))\n",
    "\n",
    "    # Perform training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            indices = list(range(n_train))\n",
    "            r.shuffle(indices)\n",
    "\n",
    "            for i in range(n_train // batch_size_train):\n",
    "                batch_indices = indices[i * batch_size_train: (i + 1) * batch_size_train]\n",
    "                batch_features = [train_set[i] for i in batch_indices]\n",
    "                batch_stances = [train_stances[i] for i in batch_indices]\n",
    "\n",
    "                batch_feed_dict = {features_pl: batch_features, stances_pl: batch_stances, keep_prob_pl: train_keep_prob}\n",
    "                _, current_loss = sess.run([opt_op, loss], feed_dict=batch_feed_dict)\n",
    "                total_loss += current_loss\n",
    "                \n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print('  Epoch, Loss = %6d: %8.3f' % (epoch, total_loss)) \n",
    "\n",
    "\n",
    "\n",
    "        # Predict\n",
    "        test_feed_dict = {features_pl: test_set, keep_prob_pl: 1.0}\n",
    "        test_pred = sess.run(predict, feed_dict=test_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "save_predictions(test_pred, file_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amld]",
   "language": "python",
   "name": "conda-env-amld-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
